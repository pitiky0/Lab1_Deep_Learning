{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom numpy import vstack\nfrom numpy import sqrt\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import MSELoss\nfrom torch.nn.init import xavier_uniform_\nfrom tqdm import tqdm\nfrom torch.nn import Module, Linear, ReLU\nfrom torch.nn.init import xavier_uniform_","metadata":{"_uuid":"2badbf7c-06a2-43f4-94fc-39d1e2a987d6","_cell_guid":"718616ea-834c-4368-816e-a13d6800d7da","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import all stock prices \ndf = pd.read_csv(\"/kaggle/input/nyse/prices.csv\", index_col = 0)\ndf.info()\ndf.head()","metadata":{"_uuid":"412f3e4d-fd3a-4855-9c9c-bbd93d146b5f","_cell_guid":"357b489d-5271-4dc4-a1a1-05b37b5f67c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vérifier les données manquantes\nprint(df.isnull().sum())","metadata":{"_uuid":"a9660f75-0a39-4625-a4c5-308e04caaf00","_cell_guid":"93faf5a5-49a9-4689-9803-f5e4b3a6435b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistiques descriptives\nprint(df.describe())","metadata":{"_uuid":"a599d9fb-f4f6-4eb3-b7f6-afa1fc841a0f","_cell_guid":"025d38d8-5497-44c5-9b09-34eb6074de56","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n# Visualisation des données\n# Calculer la matrice de corrélation\ncorr_matrix = df[['open', 'close', 'low', 'high', 'volume']].corr()\n\n# Tracer la matrice de corrélation en utilisant un heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('Matrice de Corrélation')\nplt.show()","metadata":{"_uuid":"46098120-243d-4739-be35-f1569abeb6fa","_cell_guid":"154bda49-21db-4df9-b28b-840e09ac4766","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualisation des données\nsns.pairplot(df[['open', 'close', 'low', 'high', 'volume']])\nplt.show()","metadata":{"_uuid":"04d6dcc0-5110-41d1-a69a-7648b15a3280","_cell_guid":"6d357b62-34ac-4ae3-a28c-d2f1cac583b6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch.optim as optim","metadata":{"_uuid":"18c6d109-14ee-42da-aac8-60ed6b8a129e","_cell_guid":"b8c1a10e-ed9c-40ef-b807-b2cb69a5b10f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normaliser les données\nscaler = StandardScaler()\ndf[['open', 'close', 'low', 'high', 'volume']] = scaler.fit_transform(df[['open', 'close', 'low', 'high', 'volume']])\n\n# Diviser les données en ensembles d'entraînement et de test\ntrain_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n\n# Créer des tenseurs PyTorch\nX_train = torch.tensor(train_data[['open', 'close', 'low', 'high', 'volume']].values, dtype=torch.float32)\ny_train = torch.tensor(train_data['close'].values, dtype=torch.float32)\n\n# Définir une classe de modèle\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super(RegressionModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Initialiser le modèle, la fonction de perte et l'optimiseur\nmodel = RegressionModel(input_size=5)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Entraîner le modèle\nnum_epochs = 100\nloss_values = []\n\nfor epoch in range(num_epochs):\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train.view(-1, 1))\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    loss_values.append(loss.item())\n\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')","metadata":{"_uuid":"664a039d-58f3-42a1-9a4b-51534052d94d","_cell_guid":"99ef58fb-fd90-460d-bb0f-1b9c4f7e4faa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Charger les données\ndata = pd.read_csv('/kaggle/input/nyse/prices.csv')\n\n# Normaliser les données\nscaler = StandardScaler()\ndata[['open', 'close', 'low', 'high', 'volume']] = scaler.fit_transform(data[['open', 'close', 'low', 'high', 'volume']])\n\n# Diviser les données en ensembles d'entraînement et de test\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Créer des tenseurs PyTorch pour les données d'entraînement\nX_train = torch.tensor(train_data[['open', 'close', 'low', 'high', 'volume']].values, dtype=torch.float32)\ny_train = torch.tensor(train_data['close'].values, dtype=torch.float32)\n\n# Créer une classe de modèle\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super(RegressionModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Créer une classe spéciale pour encapsuler le modèle PyTorch\nclass PyTorchRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, input_size=5, lr=0.01, epochs=10):\n        self.model = RegressionModel(input_size)\n        self.lr = lr\n        self.epochs = epochs\n        self.input_size = input_size\n\n    def fit(self, X, y):\n        criterion = nn.MSELoss()  # Définir la fonction de perte ici\n        optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n\n        for epoch in range(self.epochs):\n            outputs = self.model(X)\n            loss = criterion(outputs, y.view(-1, 1))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        return self\n\n    def predict(self, X):\n        with torch.no_grad():\n            return self.model(X).numpy()\n\n# Utiliser GridSearchCV avec moins d'epochs\nparam_grid = {\n    'lr': [0.001, 0.01, 0.1],\n    'epochs': [5, 10, 15],\n}\n\n# Utiliser GridSearchCV\nmodel_sklearn = PyTorchRegressor()\ngrid_search = GridSearchCV(model_sklearn, param_grid, scoring='neg_mean_squared_error', cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Afficher les meilleurs paramètres\nprint(\"Meilleurs paramètres:\", grid_search.best_params_)\n\n# Visualisation des courbes d'apprentissage\nX_test = torch.tensor(test_data[['open', 'close', 'low', 'high', 'volume']].values, dtype=torch.float32)\ny_test = torch.tensor(test_data['close'].values, dtype=torch.float32)\n\nmodel = grid_search.best_estimator_\nmodel.epochs = 100  # Utilisez un nombre plus grand d'epochs pour la visualisation finale\n\n# Entraîner le modèle avec enregistrement de la perte et de la précision à chaque epoch\ntrain_losses, test_losses = [], []\ncriterion = nn.MSELoss()  # Définir la fonction de perte ici aussi\nfor epoch in range(model.epochs):\n    model.fit(X_train, y_train)\n    \n    # Calculer la perte sur les données d'entraînement\n    model.model.eval()\n    train_outputs = model.predict(X_train)\n    train_loss = criterion(torch.tensor(train_outputs), y_train.view(-1, 1))\n    train_losses.append(train_loss.item())\n\n    # Calculer la perte sur les données de test\n    model.model.eval()\n    test_outputs = model.predict(X_test)\n    test_loss = criterion(torch.tensor(test_outputs), y_test.view(-1, 1))\n    test_losses.append(test_loss.item())\n\n# Visualisation des courbes d'apprentissage\nplt.figure(figsize=(12, 5))\n\n# Courbe de perte\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Courbe de Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"3055c319-5c96-4a40-8463-aabab96baabc","_cell_guid":"b5658a65-7c4c-4b91-a31c-e1d852a863d6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of different stocks\nprint('\\nnumber of different stocks: ', len(list(set(df.symbol))))\nprint(list(set(df.symbol))[:10])","metadata":{"_uuid":"de81d49e-becb-4ddf-996a-40b879fe9a53","_cell_guid":"e25c6fe3-3d1e-4c8b-b0f6-972b0ee7937f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].low.values, color='blue', label='low')\nplt.plot(df[df.symbol == 'EQIX'].high.values, color='black', label='high')\nplt.title('stock price')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend(loc='best')\n\nplt.subplot(1,2,2);\nplt.plot(df[df.symbol == 'EQIX'].volume.values, color='green', label='volume')\nplt.title('stock volume')\nplt.xlabel('time [days]')\nplt.ylabel('volume')\nplt.legend(loc='best')\n#plt.show()","metadata":{"_uuid":"0bea74cf-ae52-403b-89d1-cdfffdccf2bd","_cell_guid":"796f9752-da51-4ad1-b5ad-fdcceb1b3bb8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nimport numpy as np \ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # calculate absolute mean squared error\n        abs_mse = np.mean(np.abs(yhat - actual)**2)\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    return abs_mse","metadata":{"_uuid":"e3b31a8b-dfdf-4da4-8632-001de6cf7b1f","_cell_guid":"36765db9-e22e-4916-b8a8-0e2e0927d93f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn import Module, Linear, ReLU, MSELoss\nfrom torch.nn.init import xavier_uniform_\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom pandas import read_csv\nfrom numpy import vstack\nimport matplotlib.pyplot as plt\n\n# Model definition\nclass MLP(Module):\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        self.hidden1 = Linear(n_inputs, 10)\n        xavier_uniform_(self.hidden1.weight)\n        self.act1 = ReLU()\n        self.hidden2 = Linear(10, 8)\n        xavier_uniform_(self.hidden2.weight)\n        self.act2 = ReLU()\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n\n    def forward(self, X):\n        X = self.hidden1(X)\n        X = self.act1(X)\n        X = self.hidden2(X)\n        X = self.act2(X)\n        X = self.hidden3(X)\n        return X\n\n# Dataset class\nclass CSVDataset(Dataset):\n    def __init__(self, df, symbol):\n        df_stock = df[df['symbol'] == symbol]\n        self.X = df_stock.values[:, 1:].astype('float32')  # Exclude 'symbol' column\n        self.y = df_stock['close'].values.astype('float32').reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    def get_splits(self, test_size=0.2):\n        X_train, X_test, y_train, y_test = train_test_split(\n            self.X, self.y, test_size=test_size, random_state=42)\n        return (\n            DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True),\n            DataLoader(list(zip(X_test, y_test)), batch_size=1024, shuffle=False)\n        )\n\n# Training function\n# Training function with loss and accuracy tracking\ndef train_model(train_dl, test_dl, model, optimizer, criterion):\n    size = len(train_dl.dataset)\n    train_losses, test_losses = [], []\n    train_accuracies, test_accuracies = [], []\n\n    for epoch in tqdm(range(100), desc='Training Epochs'):\n        model.train()\n        train_loss, correct_train = 0, 0\n        for batch, (inputs, targets) in enumerate(train_dl):\n            optimizer.zero_grad()\n            yhat = model(inputs)\n            loss = criterion(yhat, targets)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * len(inputs)\n            correct_train += ((yhat > 0.5) == targets).sum().item()\n\n        train_losses.append(train_loss / size)\n        train_accuracies.append(correct_train / size)\n\n        # Evaluate on the test set\n        model.eval()\n        test_loss, correct_test = 0, 0\n        with torch.no_grad():\n            for inputs, targets in test_dl:\n                yhat = model(inputs)\n                loss = criterion(yhat, targets)\n\n                test_loss += loss.item() * len(inputs)\n                correct_test += ((yhat > 0.5) == targets).sum().item()\n\n        test_losses.append(test_loss / len(test_dl.dataset))\n        test_accuracies.append(correct_test / len(test_dl.dataset))\n\n    return train_losses, test_losses, train_accuracies, test_accuracies\n\n# Evaluation function\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        yhat = model(inputs)\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy().reshape(-1, 1)\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    return predictions, actuals\n\n# Calculate accuracy\ndef calculate_accuracy(predictions, actuals, threshold=0.5):\n    binary_predictions = (predictions > threshold).astype(int)\n    binary_actuals = (actuals > threshold).astype(int)\n    accuracy = (binary_predictions == binary_actuals).mean()\n    return accuracy\n\n# Load the dataset\nsymbol_of_interest = 'ORCL'\ndataset = CSVDataset(df, symbol_of_interest)\ntrain_dl, test_dl = dataset.get_splits()\n\n# Model, optimizer, and criterion\nmodel = MLP(n_inputs=dataset.X.shape[1])\noptimizer = Adam(model.parameters(), lr=0.001)\ncriterion = MSELoss()\n\n# Train the model\ntrain_model(train_dl, model, optimizer, criterion)\n\n# Evaluate the model\npredictions, actuals = evaluate_model(test_dl, model)\n\n# Print accuracy\naccuracy = calculate_accuracy(predictions, actuals)\nprint(f\"Accuracy on the test set: {accuracy:.2%}\")","metadata":{"_uuid":"fc645bba-f267-478a-8b8d-25453f6deb20","_cell_guid":"0187490b-990a-4585-a765-e9991343e1a5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn import Module, Linear, ReLU, BCEWithLogitsLoss\nfrom torch.nn.init import xavier_uniform_\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Model definition\nclass MLP(Module):\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        self.hidden1 = Linear(n_inputs, 64)\n        xavier_uniform_(self.hidden1.weight)\n        self.act1 = ReLU()\n        self.hidden2 = Linear(64, 32)\n        xavier_uniform_(self.hidden2.weight)\n        self.act2 = ReLU()\n        self.hidden3 = Linear(32, 1)\n        xavier_uniform_(self.hidden3.weight)\n\n    def forward(self, X):\n        X = self.hidden1(X)\n        X = self.act1(X)\n        X = self.hidden2(X)\n        X = self.act2(X)\n        X = self.hidden3(X)\n        return X\n\n# Dataset class\nclass CSVDataset(Dataset):\n    def __init__(self, df, symbol):\n        df_stock = df[df['symbol'] == symbol]\n        self.X = df_stock[['open', 'low', 'high', 'volume']].values.astype('float32')\n        self.y = (df_stock['close'].pct_change() > 0).astype('float32').values.reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    def get_splits(self, test_size=0.2):\n        X_train, X_test, y_train, y_test = train_test_split(\n            self.X, self.y, test_size=test_size, random_state=42)\n        return (\n            DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True),\n            DataLoader(list(zip(X_test, y_test)), batch_size=1024, shuffle=False)\n        )\n\n# Training function with loss and accuracy tracking\ndef train_model(train_dl, test_dl, model, optimizer, criterion, device):\n    size = len(train_dl.dataset)\n    train_losses, test_losses = [], []\n    train_accuracies, test_accuracies = [], []\n\n    for epoch in tqdm(range(100), desc='Training Epochs'):\n        model.train()\n        train_loss, correct_train = 0, 0\n        for inputs, targets in train_dl:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            yhat = model(inputs)\n            loss = criterion(yhat, targets)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * len(inputs)\n            correct_train += ((yhat > 0.5) == targets).sum().item()\n\n        train_losses.append(train_loss / size)\n        train_accuracies.append(correct_train / size)\n\n        # Evaluate on the test set\n        model.eval()\n        test_loss, correct_test = 0, 0\n        with torch.no_grad():\n            for inputs, targets in test_dl:\n                inputs, targets = inputs.to(device), targets.to(device)\n                yhat = model(inputs)\n                loss = criterion(yhat, targets)\n\n                test_loss += loss.item() * len(inputs)\n                correct_test += ((yhat > 0.5) == targets).sum().item()\n\n        test_losses.append(test_loss / len(test_dl.dataset))\n        test_accuracies.append(correct_test / len(test_dl.dataset))\n\n    return train_losses, test_losses, train_accuracies, test_accuracies\n\n# Load the dataset\nsymbol_of_interest = 'ORCL'\ndataset = CSVDataset(df, symbol_of_interest)\ntrain_dl, test_dl = dataset.get_splits()\n\n# Model, optimizer, and criterion\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLP(n_inputs=dataset.X.shape[1]).to(device)\noptimizer = Adam(model.parameters(), lr=0.001)\ncriterion = BCEWithLogitsLoss()\n\n# Train the model and get the loss and accuracy values\ntrain_losses, test_losses, train_accuracies, test_accuracies = train_model(train_dl, test_dl, model, optimizer, criterion, device)\n\n# Plot Loss / Epochs\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\nplt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot Accuracy / Epochs\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\nplt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\n# Evaluate additional metrics\npredictions, actuals = evaluate_model(test_dl, model)\naccuracy = accuracy_score(actuals, (predictions > 0.5).astype(int))\nprecision = precision_score(actuals, (predictions > 0.5).astype(int))\nrecall = recall_score(actuals, (predictions > 0.5).astype(int))\nf1 = f1_score(actuals, (predictions > 0.5).astype(int))\n\nprint(f\"Accuracy on the test set: {accuracy:.2%}\")\nprint(f\"Precision: {precision:.2%}\")\nprint(f\"Recall: {recall:.2%}\")\nprint(f\"F1 Score: {f1:.2%}\")","metadata":{"_uuid":"e979a3f4-0b83-42d5-9f3d-d80e2ec0a28d","_cell_guid":"dcf92c3c-3102-458f-a0c2-85d2c62b149a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c2404607-3207-4855-9fac-9371c8f9bc19","_cell_guid":"2ca8577d-abe7-426b-ae92-ba9ce010ac5f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}